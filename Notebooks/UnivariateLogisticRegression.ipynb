{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add path to custom modules\n",
    "import sys\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/Library/TeX/texbin'\n",
    "this_path = os.path.abspath('') \n",
    "parent_dir = os.path.dirname(this_path)  \n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Import custom modules\n",
    "from Modules import Parameters, DataPreprocessing as DP, Classification as CL, CustomFunctions as CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multivariate sets\n",
    "immunecells_set = Parameters.immunecells_set\n",
    "FC_set = Parameters.FC_set\n",
    "Dem_set = Parameters.Dem_set\n",
    "CK_set = Parameters.CK_set\n",
    "BM_set =  Parameters.BM_set\n",
    "allinput_set = Parameters.allinput_set\n",
    "\n",
    "## Targets\n",
    "target_train = Parameters.train_target\n",
    "target_test = Parameters.test_target\n",
    "\n",
    "## Minimum NPV\n",
    "min_NPV_Models = Parameters.min_NPV_Models\n",
    "min_NPV = Parameters.min_NPV\n",
    "\n",
    "## Nan masking row-wise\n",
    "do_nan_masking = Parameters.do_nan_masking\n",
    "do_nan_masking_univ = Parameters.do_nan_masking_univ\n",
    "nan_masking = Parameters.nan_masking\n",
    "do_nan_masking_groupwise = False # False: filter nans independently for each variable\n",
    "\n",
    "## N samples for average\n",
    "N_av = Parameters.N_av\n",
    "\n",
    "## Imputation\n",
    "imputation_method = Parameters.imputation_method\n",
    "imputation_method_univ = Parameters.imputation_method_univ\n",
    "\n",
    "## Standardization\n",
    "std_method = Parameters.std_method\n",
    "\n",
    "## PCA % var. threshold\n",
    "pc_var_th = Parameters.pca_var_threshold\n",
    "\n",
    "## Preprocessing\n",
    "do_preprocessing = Parameters.do_preprocessing_univ\n",
    "\n",
    "## Train-test\n",
    "test_size = Parameters.test_size\n",
    "\n",
    "## Paths\n",
    "path_results = Parameters.path_results\n",
    "path_figures = Parameters.path_figures\n",
    "exp_description = Parameters.exp_description\n",
    "exp_univ_description = Parameters.exp_univ_description\n",
    "foldername_univ = Parameters.foldername_univ\n",
    "\n",
    "## Run experiments\n",
    "run_experiments = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataInpatients, DataOutpatients = DP.data_preprocessing()\n",
    "\n",
    "# Mask data by target\n",
    "mask = pd.notnull(DataInpatients[target_train].values) & pd.notnull(DataInpatients[target_test].values)\n",
    "DataInpatients = DataInpatients.loc[mask,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "\n",
    "## LR models\n",
    "models_dict['LR'] = {}\n",
    "for variable in allinput_set:\n",
    "    models_dict['LR'][variable] = [variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis - multiple splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    Data = DataInpatients.copy()     \n",
    "    \n",
    "    ## Initialize results dictionaries\n",
    "    Results_N_av = {}\n",
    "    Ground_Truth_N_av = {}\n",
    "    for model in models_dict.keys():\n",
    "        Results_N_av[model] = {}\n",
    "        Ground_Truth_N_av[model] = {}\n",
    "        for set_name in models_dict[model].keys():\n",
    "            Results_N_av[model][set_name] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': [], 'Weights':[], 'Bias':[], 'C':[], 'Std_Parameters': []}\n",
    "            Ground_Truth_N_av[model][set_name] = {'Train':[], 'Test':[]}\n",
    "            if min_NPV_Models:\n",
    "                Results_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': [], 'Cutoff': []}\n",
    "                Ground_Truth_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[]}\n",
    "                 \n",
    "\n",
    "    ## Sets of variables for nan removal\n",
    "    groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "\n",
    "\n",
    "    ## Run average\n",
    "    fix_outliers = False\n",
    "    do_imputation = False\n",
    "    print('Running average...')\n",
    "    print('Fix outliers: %s' % fix_outliers)\n",
    "    print('Do imputation: %s' % do_imputation)\n",
    "    print('Nan masking groupwise: %s' % do_nan_masking_groupwise)\n",
    "    print('Do preprocessing: %s' % do_preprocessing)\n",
    "    for i_split in range(N_av): \n",
    "        idx_split = i_split+1\n",
    "        print('%d/%d' %(idx_split, N_av))\n",
    "        \n",
    "        Results = CL.models_prediction(Data=Data.copy(),\n",
    "                                       test_size=test_size,\n",
    "                                       models_dict=models_dict,\n",
    "                                       target_train=target_train, \n",
    "                                       target_test=target_test, \n",
    "                                       min_NPV=min_NPV,\n",
    "                                       min_NPV_Models=min_NPV_Models,\n",
    "                                       standardization=std_method, \n",
    "                                       imputation=imputation_method, \n",
    "                                       do_nan_masking=do_nan_masking,\n",
    "                                       do_nan_masking_univ=do_nan_masking_univ,\n",
    "                                       nan_masking=nan_masking, \n",
    "                                       do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                       groups=groups,\n",
    "                                       do_preprocessing=do_preprocessing, \n",
    "                                       fix_outliers=fix_outliers,\n",
    "                                       do_imputation=do_imputation, \n",
    "                                       random_state=idx_split)\n",
    "\n",
    "        for model in Results_N_av.keys():\n",
    "            for set_name in Results_N_av[model].keys():\n",
    "                Ground_Truth_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train_Labels'])\n",
    "                Ground_Truth_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test_Labels'])\n",
    "                Results_N_av[model][set_name]['Train_value'].append(Results[model][set_name]['Train_value'])\n",
    "                Results_N_av[model][set_name]['Test_value'].append(Results[model][set_name]['Test_value'])\n",
    "                Results_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train'])\n",
    "                Results_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test'])\n",
    "                \n",
    "                if model=='LR' and 'minNPV' in set_name:\n",
    "                    Results_N_av[model][set_name]['Cutoff'].append(Results[model][set_name]['Cutoff'])\n",
    "\n",
    "                if model=='LR' and '_minNPV' not in set_name:\n",
    "                    Results_N_av[model][set_name]['Weights'].append(Results[model][set_name]['Weights'])\n",
    "                    Results_N_av[model][set_name]['Bias'].append(Results[model][set_name]['Bias'])\n",
    "                    Results_N_av[model][set_name]['C'].append(Results[model][set_name]['C'])\n",
    "                    Results_N_av[model][set_name]['Std_Parameters'].append(Results[model][set_name]['Std_Parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    Performances = {}\n",
    "\n",
    "    Scores = {'roc_auc_score': roc_auc_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'NPV': precision_score,\n",
    "              'specificity': recall_score,\n",
    "              'accuracy': accuracy_score}\n",
    "\n",
    "    for model in Results_N_av.keys():\n",
    "        Performances[model] = {}\n",
    "\n",
    "        for set_name in Results_N_av[model].keys():\n",
    "            #print(model, set_name)\n",
    "            Performances[model][set_name] = {}\n",
    "            print(model, set_name)\n",
    "\n",
    "            for score in Scores.keys():\n",
    "                Performances[model][set_name][score] = {'Train': [], 'Test': []}\n",
    "                sc = Scores[score]\n",
    "                for sample in range(N_av):\n",
    "                    y_pred_train = Results_N_av[model][set_name]['Train'][sample]\n",
    "                    y_value_train = Results_N_av[model][set_name]['Train_value'][sample]\n",
    "                    y_train = Ground_Truth_N_av[model][set_name]['Train'][sample]\n",
    "                    y_pred_test = Results_N_av[model][set_name]['Test'][sample]\n",
    "                    y_value_test = Results_N_av[model][set_name]['Test_value'][sample]\n",
    "                    y_test = Ground_Truth_N_av[model][set_name]['Test'][sample]\n",
    "\n",
    "                    y_pred_train_1 = 1 - y_pred_train\n",
    "                    y_train_1 = 1 - y_train\n",
    "                    y_pred_test_1 = 1 - y_pred_test\n",
    "                    y_test_1 = 1 - y_test\n",
    "\n",
    "                    if score=='NPV':\n",
    "                        if sum(pd.notnull(y_pred_train_1))>1:\n",
    "                            score_val_train = sc(y_train_1, y_pred_train_1, zero_division=0)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test_1, y_pred_test_1, zero_division=0)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    elif score=='specificity':\n",
    "                        if sum(pd.notnull(y_pred_train_1))>1:\n",
    "                            score_val_train = sc(y_train_1, y_pred_train_1)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test_1, y_pred_test_1)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    elif score=='roc_auc_score':\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_value_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test))>1:\n",
    "                            score_val_test = sc(y_test, y_value_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    elif score=='precision':\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_pred_train, zero_division=0)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test))>1:\n",
    "                            score_val_test = sc(y_test, y_pred_test, zero_division=0)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    else:\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_pred_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test))>1:\n",
    "                            score_val_test = sc(y_test, y_pred_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    Performances[model][set_name][score]['Train'].append(score_val_train)\n",
    "                    Performances[model][set_name][score]['Test'].append(score_val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    # Train performances\n",
    "    test_or_train = 'Train'\n",
    "    Performances_mean_ci_train = {}\n",
    "    Performances_mean_ci_train_0 = {}\n",
    "    notnull_prop = 0.9\n",
    "    normality_pval_th = 0.001\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_ci_train[score] = {'Mean':[], 'CI_lower':[], 'CI_upper':[]}\n",
    "        Performances_mean_ci_train_0[score] = {'Mean':[], 'CI_lower':[], 'CI_upper':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                mask = pd.notnull(v)\n",
    "                if sum(mask)>notnull_prop*len(mask):\n",
    "                    v = v[pd.notnull(v)]\n",
    "                    # Normality test\n",
    "                    normality_pval = st.normaltest(v, nan_policy='omit')[1]\n",
    "                    if normality_pval>normality_pval_th:\n",
    "                        ci_lower, ci_upper = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(v)\n",
    "                    else:\n",
    "                        ci_lower, ci_upper, score_mean = CF.bootstrap_ci_mean(v)\n",
    "                else:\n",
    "                    ci_upper = 0\n",
    "                    ci_lower = 0 \n",
    "                    score_mean = 0\n",
    "\n",
    "                if '_minNPV' not in set_name:\n",
    "                    Performances_mean_ci_train[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_ci_train[score]['CI_lower'].append(ci_lower)\n",
    "                    Performances_mean_ci_train[score]['CI_upper'].append(ci_upper)\n",
    "                else:\n",
    "                    Performances_mean_ci_train_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_ci_train_0[score]['CI_lower'].append(ci_lower)\n",
    "                    Performances_mean_ci_train_0[score]['CI_upper'].append(ci_upper)\n",
    "\n",
    "\n",
    "    # Test performances\n",
    "    test_or_train = 'Test'\n",
    "    Performances_mean_ci_test = {}\n",
    "    Performances_mean_ci_test_0 = {}\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_ci_test[score] = {'Mean':[], 'CI_lower':[], 'CI_upper':[]}\n",
    "        Performances_mean_ci_test_0[score] = {'Mean':[], 'CI_lower':[], 'CI_upper':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                mask = pd.notnull(v)\n",
    "                if sum(mask)>notnull_prop*len(mask):\n",
    "                    v = v[pd.notnull(v)]\n",
    "                    # Normality test\n",
    "                    normality_pval = st.normaltest(v, nan_policy='omit')[1]\n",
    "                    if normality_pval>normality_pval_th:\n",
    "                        ci_lower, ci_upper = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(v)\n",
    "                    else:\n",
    "                        ci_lower, ci_upper, score_mean = CF.bootstrap_ci_mean(v)\n",
    "                else:\n",
    "                    ci_upper = 0\n",
    "                    ci_lower = 0 \n",
    "                    score_mean = 0\n",
    "\n",
    "                if '_minNPV' not in set_name:\n",
    "                    Performances_mean_ci_test[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_ci_test[score]['CI_lower'].append(ci_lower)\n",
    "                    Performances_mean_ci_test[score]['CI_upper'].append(ci_upper)\n",
    "                else:\n",
    "                    Performances_mean_ci_test_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_ci_test_0[score]['CI_lower'].append(ci_lower)\n",
    "                    Performances_mean_ci_test_0[score]['CI_upper'].append(ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average cutoff thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    cutoff_mean_ci_train = {'Mean':[], 'CI_lower':[], 'CI_upper':[]}\n",
    "\n",
    "    test_or_train = 'Train'\n",
    "\n",
    "    for model in Results_N_av.keys():\n",
    "        for set_name in Results_N_av[model].keys():\n",
    "\n",
    "            if 'minNPV' in set_name:\n",
    "                v = np.array(Results_N_av[model][set_name]['Cutoff'])\n",
    "                mask = pd.notnull(v)\n",
    "                if sum(mask)>notnull_prop*len(mask):\n",
    "                    v = v[pd.notnull(v)]\n",
    "                    # Normality test\n",
    "                    normality_pval = st.normaltest(v, nan_policy='omit')[1]\n",
    "                    if normality_pval>normality_pval_th:\n",
    "                        ci_lower, ci_upper = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        cutoff_mean = np.mean(v)\n",
    "                    else:\n",
    "                        ci_lower, ci_upper, cutoff_mean = CF.bootstrap_ci_mean(v)\n",
    "                else:\n",
    "                    ci_upper = 0\n",
    "                    ci_lower = 0 \n",
    "                    cutoff_mean = 0\n",
    "\n",
    "                cutoff_mean_ci_train['Mean'].append(cutoff_mean)\n",
    "                cutoff_mean_ci_train['CI_lower'].append(ci_lower)\n",
    "                cutoff_mean_ci_train['CI_upper'].append(ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    # Create directory\n",
    "    try:\n",
    "        os.mkdir(path_results+foldername_univ)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory failed\")\n",
    "    else:\n",
    "        print (\"Successfully created the directory\")\n",
    "\n",
    "    path_export = path_results+foldername_univ\n",
    "\n",
    "    # Export \n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, CI_lower) (name_score, CI_upper) #\n",
    "    my_tuples = [(score, val) for score in Scores for val in ['Mean', 'CI_lower', 'CI_upper']]\n",
    "    new_columns = pd.MultiIndex.from_tuples(my_tuples)\n",
    "    index = []\n",
    "    for model in models_dict.keys():\n",
    "        index.extend([model+': '+name for name in models_dict[model].keys()])\n",
    "    n_models = len(index)\n",
    "    values = np.array([np.array(Performances_mean_ci_train[score][val]) for score in Scores for val in ['Mean', 'CI_lower', 'CI_upper']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_train.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_ci_train_0[score][val]) for score in Scores for val in ['Mean', 'CI_lower', 'CI_upper']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_train.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, CI_lower) (name_score, CI_upper) #\n",
    "    values = np.array([np.array(Performances_mean_ci_test[score][val]) for score in Scores for val in ['Mean', 'CI_lower', 'CI_upper']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_test.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_ci_test_0[score][val]) for score in Scores for val in ['Mean', 'CI_lower', 'CI_upper']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_test.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "    # Export cutoffs\n",
    "    if min_NPV_Models:\n",
    "        models_list = [name for name in Results_N_av[model].keys() if '_minNPV' not in name]\n",
    "        df_cutoff = pd.DataFrame(cutoff_mean_ci_train, index=models_list)\n",
    "        filename_0 = 'cutoffs_minNPV_test.xlsx'\n",
    "        df_cutoff.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "    # Export models description\n",
    "    models_description = {}\n",
    "    for model in models_dict.keys():\n",
    "        for name in models_dict[model].keys():\n",
    "            description = ' # '.join(models_dict[model][name])\n",
    "            models_description[model+'#'+name] = description\n",
    "\n",
    "    filename = 'models_description.xlsx'\n",
    "    pd.Series(models_description).to_excel(path_export+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color scheme\n",
    "color_0 = Parameters.dark_blue\n",
    "color_1 = Parameters.red_purp\n",
    "color_control = Parameters.green\n",
    "\n",
    "# Parameters\n",
    "fontsize = 9\n",
    "ylabelsize = 7\n",
    "xlabelsize = 7 \n",
    "tex = True\n",
    "axes_lines_w = 0.5\n",
    "lines_w = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'RTE/uL' # 'RTE/uL', 'IP10', 'proADM' \n",
    "\n",
    "Data_train = DataInpatients.copy()\n",
    "Data_test = DataOutpatients.copy() # Data_Control\n",
    "if model_name not in immunecells_set:\n",
    "    Data_test = DataInpatients.copy() \n",
    "    \n",
    "print(Data_train.shape, Data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = None\n",
    "columns = models_dict['LR'][model_name]\n",
    "fix_outliers = False\n",
    "do_imputation = True\n",
    "groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "#columns = list(np.random.permutation(columns))\n",
    "\n",
    "results, MLR, prep_data = CL.LR_model_results(Data=Data_train,\n",
    "                                              Data_test=Data_test,\n",
    "                                              features=columns, \n",
    "                                              set_name=model_name, \n",
    "                                              target_train=target_train, \n",
    "                                              target_test=target_test, \n",
    "                                              test_size=test_size,\n",
    "                                              hyperp_dict={}, \n",
    "                                              do_preprocessing=do_preprocessing, \n",
    "                                              fix_outliers=fix_outliers, \n",
    "                                              do_imputation=do_imputation, \n",
    "                                              imputation=imputation_method, \n",
    "                                              pca_var_threshold=pc_var_th, \n",
    "                                              standardization=std_method, \n",
    "                                              min_NPV_Models=min_NPV_Models,\n",
    "                                              min_NPV=min_NPV, \n",
    "                                              groups=groups, \n",
    "                                              do_nan_masking=do_nan_masking, \n",
    "                                              do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                              do_nan_masking_univ=do_nan_masking_univ, \n",
    "                                              nan_masking=nan_masking,\n",
    "                                              return_model=True, \n",
    "                                              return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data histogram\n",
    "Data_X = prep_data['Train']\n",
    "y_train = results[model_name]['Train_Labels']\n",
    "y_pred = results[model_name]['Train']\n",
    "value_pred = results[model_name]['Train_value']\n",
    "std = StandardScaler()\n",
    "value_pred = std.fit_transform(value_pred.reshape(-1, 1)).reshape(-1,)\n",
    "x_test = results[model_name]['Test_value']\n",
    "x_test = std.transform(x_test.reshape(-1, 1)).reshape(-1,)\n",
    "mask_0 = y_train==0\n",
    "mask_1 = y_train==1\n",
    "x0 = value_pred[mask_0]\n",
    "x1 = value_pred[mask_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnification = 0.85\n",
    "ratio = 2.4/3.7\n",
    "CF.SetPlotParams(magnification=magnification, ratio=ratio, \n",
    "                        fontsize=fontsize, ylabelsize=ylabelsize, xlabelsize=xlabelsize, \n",
    "                        tex=tex, axes_lines_w=axes_lines_w, lines_w=lines_w)\n",
    "mec = 'white'\n",
    "ms = 9\n",
    "mew = 0.15\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "y_target = y_train\n",
    "th = CL.best_threshold_class0(y_pred=y_pred, value_pred=value_pred, y_target=y_train, min_NPV=0.96, fixed_threshold=False)\n",
    "y_pred = np.zeros_like(y_target)\n",
    "y_pred[value_pred>=th] = 1\n",
    "y_pred_1 = 1 - y_pred\n",
    "y_target_1 = 1 - y_target\n",
    "x_min = min([np.nanmin(x0), np.nanmin(x1), np.nanmin(x_test)])\n",
    "x_max = max([np.nanmax(x0), np.nanmax(x1), np.nanmax(x_test)])\n",
    "\n",
    "if model_name in immunecells_set:\n",
    "    x_control = x_test\n",
    "    print('control: %d%%' % (round(100*np.sum(x_control<th)/len(x_control))))\n",
    "\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 14)\n",
    "\n",
    "color = color_0\n",
    "density = False\n",
    "ax.hist(x0, bins,\n",
    "        alpha=0.15,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density)\n",
    "ax.hist(x0, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None', \n",
    "        label='no event')\n",
    "\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 13)\n",
    "\n",
    "if model_name in immunecells_set:\n",
    "    color = color_control\n",
    "    ax.hist(x_control, bins,\n",
    "            alpha=0.2,\n",
    "            histtype='stepfilled',\n",
    "            color=color,\n",
    "            density=density,\n",
    "            label='control')\n",
    "    ax.hist(x_control, bins,\n",
    "            alpha=1.,\n",
    "            histtype='step',\n",
    "            edgecolor=color,\n",
    "            density=density,\n",
    "            facecolor='None')\n",
    "\n",
    "color = color_1\n",
    "ax.hist(x1, bins,\n",
    "        alpha=0.2,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density,\n",
    "        label='event')\n",
    "ax.hist(x1, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None')\n",
    "\n",
    "color_boundary = CF.lighten_color(Parameters.grey_1, 0.6)\n",
    "ax.axvline(th, ls='--', lw=0.7, color='black') \n",
    "\n",
    "ax.title.set_text('Score Histogram')\n",
    "ax.set_xlabel('%s score' % (CF.change_names([model_name])[0]))\n",
    "ax.set_ylabel('n. patients')\n",
    "if density:\n",
    "    ax.set_ylabel('patients (density)')\n",
    "\n",
    "\n",
    "if model_name=='RTE/uL':\n",
    "        ax.set_yticks([0., 45., 90])\n",
    "        ax.set_ylim([0, 90])\n",
    "if model_name=='proADM':\n",
    "        ax.set_yticks([0., 40., 80.])\n",
    "        ax.set_ylim([0, 80])\n",
    "if model_name=='IP10':\n",
    "        ax.set_yticks([0., 20., 40.])\n",
    "        ax.set_ylim([0, 40])\n",
    "\n",
    "\n",
    "leg = ax.legend(loc='center left', bbox_to_anchor=(0.98, 0.5), frameon=False) #legend(loc=0, frameon=False)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    lh.set_edgecolor('white')\n",
    "\n",
    "filename = 'HistogramLR#%s' % (CF.change_names([model_name])[0])\n",
    "filename = filename + exp_description + exp_univ_description + '.pdf'\n",
    "saving_str = path_figures + filename\n",
    "#plt.savefig(saving_str, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_6_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
