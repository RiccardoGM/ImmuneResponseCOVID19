{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/Library/TeX/texbin'\n",
    "sys.path.append('/Users/riccardo/Documents/GitHub/COVID19Classification/')\n",
    "\n",
    "from Modules import Parameters, DataPreprocessing as DP, Classification as CL, CustomFunctions as CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multivariate sets\n",
    "immunecells_set = Parameters.immunecells_set\n",
    "FC_set = Parameters.FC_set\n",
    "Dem_set = Parameters.Dem_set\n",
    "CK_set = Parameters.CK_set\n",
    "BM_set =  Parameters.BM_set\n",
    "allinput_set = Parameters.allinput_set\n",
    "## Targets\n",
    "target_train = Parameters.train_target\n",
    "target_test = Parameters.test_target\n",
    "\n",
    "## Age\n",
    "lower_bound = Parameters.age_min\n",
    "upper_bound = Parameters.age_max\n",
    "\n",
    "## Delta onset\n",
    "lower_bound_donset = Parameters.donset_min\n",
    "upper_bound_donset = Parameters.donset_max\n",
    "\n",
    "## Minimum NPV\n",
    "min_NPV_Models = Parameters.min_NPV_Models\n",
    "min_NPV = Parameters.min_NPV\n",
    "\n",
    "## Correlation threshod\n",
    "corr_th_univ = Parameters.corr_th_univ\n",
    "\n",
    "## Min % no nans per column\n",
    "perc_nonans = Parameters.perc_nonans\n",
    "perc_nonans_univ = Parameters.perc_nonans_univ\n",
    "\n",
    "## Nan masking row-wise\n",
    "do_nan_masking = Parameters.do_nan_masking\n",
    "do_nan_masking_univ = Parameters.do_nan_masking_univ\n",
    "nan_masking = Parameters.nan_masking\n",
    "do_nan_masking_groupwise = False #Parameters.do_nan_masking_groupwise\n",
    "\n",
    "## Reference time\n",
    "ref_time = Parameters.ref_time\n",
    "\n",
    "## N samples for average\n",
    "N_av = Parameters.N_av\n",
    "\n",
    "## Imputation\n",
    "imputation_method = Parameters.imputation_method\n",
    "imputation_method_univ = Parameters.imputation_method_univ\n",
    "\n",
    "## Standardization\n",
    "std_method = Parameters.std_method\n",
    "\n",
    "## PCA % var. threshold\n",
    "pc_var_th = Parameters.pca_var_threshold\n",
    "\n",
    "## Preprocessing\n",
    "do_preprocessing = Parameters.do_preprocessing_univ\n",
    "\n",
    "## Train-test\n",
    "test_size = Parameters.test_size\n",
    "\n",
    "## Plot\n",
    "plot_minNPV_models = Parameters.plot_minNPV_models\n",
    "\n",
    "## Feature selection\n",
    "use_manual_selection = Parameters.use_manual_selection\n",
    "\n",
    "## Paths\n",
    "path_datasets = Parameters.path_datasets\n",
    "path_results = Parameters.path_results\n",
    "path_figures = Parameters.path_figures\n",
    "exp_description = Parameters.exp_description\n",
    "exp_univ_description = Parameters.exp_univ_description\n",
    "exp_multiv_description = Parameters.exp_multiv_description\n",
    "foldername_univ = Parameters.foldername_univ\n",
    "foldername_multiv = Parameters.foldername_multiv\n",
    "\n",
    "## Dataset\n",
    "use_CCIMasked_dataset = Parameters.use_CCIMasked_dataset\n",
    "\n",
    "## Regulariser\n",
    "find_regulariser_before_average = Parameters.find_regulariser_before_average\n",
    "\n",
    "## Run experiments\n",
    "run_experiments = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataInpatients, DataOutpatients = DP.data_preprocessing()\n",
    "\n",
    "# Mask data by target\n",
    "mask = pd.notnull(DataInpatients[target_train].values) & pd.notnull(DataInpatients[target_test].values)\n",
    "DataInpatients = DataInpatients.loc[mask,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "\n",
    "## LR models\n",
    "models_dict['LR'] = {}\n",
    "for variable in allinput_set:\n",
    "    models_dict['LR'][variable] = [variable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run analysis - multiple splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    Data = DataInpatients.copy()     \n",
    "    \n",
    "    ## Initialize results dictionaries\n",
    "    Results_N_av = {}\n",
    "    Ground_Truth_N_av = {}\n",
    "    for model in models_dict.keys():\n",
    "        Results_N_av[model] = {}\n",
    "        Ground_Truth_N_av[model] = {}\n",
    "        for set_name in models_dict[model].keys():\n",
    "            Results_N_av[model][set_name] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': [], 'Weights':[], 'Bias':[], 'C':[], 'Std_Parameters': []}\n",
    "            Ground_Truth_N_av[model][set_name] = {'Train':[], 'Test':[]}\n",
    "            if min_NPV_Models:\n",
    "                Results_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': [], 'Cutoff': []}\n",
    "                Ground_Truth_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[]}\n",
    "    \n",
    "    do_statistic_misclassified = True\n",
    "    if do_statistic_misclassified:\n",
    "        positives = Data.loc[Data[target_test].values==1, 'ID'].values\n",
    "        negatives = Data.loc[Data[target_test].values==0, 'ID'].values\n",
    "        Negatives_N_av = {}\n",
    "        False_positives_N_av = {}\n",
    "        Positives_N_av = {}\n",
    "        False_negatives_N_av = {}\n",
    "        for model in models_dict.keys():\n",
    "            model_set_names = models_dict[model].keys()\n",
    "            Negatives_N_av[model] = pd.DataFrame(np.zeros((len(negatives), len(model_set_names))), index=negatives, columns=model_set_names)\n",
    "            False_positives_N_av[model]= pd.DataFrame(np.zeros((len(negatives), len(model_set_names))), index=negatives, columns=model_set_names)\n",
    "            Positives_N_av[model] = pd.DataFrame(np.zeros((len(positives), len(model_set_names))), index=positives, columns=model_set_names)\n",
    "            False_negatives_N_av[model] = pd.DataFrame(np.zeros((len(positives), len(model_set_names))), index=positives, columns=model_set_names)\n",
    "            \n",
    "\n",
    "    ## Sets of variables for nan removal\n",
    "    groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "\n",
    "\n",
    "    ## Run average\n",
    "    fix_outliers = False\n",
    "    do_imputation = False\n",
    "    print('Running average...')\n",
    "    print('Fix outliers: %s' % fix_outliers)\n",
    "    print('Do imputation: %s' % do_imputation)\n",
    "    print('Nan masking groupwise: %s' % do_nan_masking_groupwise)\n",
    "    print('Do preprocessing: %s' % do_preprocessing)\n",
    "    for i_split in range(N_av): \n",
    "        print('%d/%d' %(i_split+1, N_av))\n",
    "        \n",
    "        Results = CL.models_prediction(Data=Data.copy(),\n",
    "                                       test_size=test_size,\n",
    "                                       models_dict=models_dict,\n",
    "                                       target_train=target_train, \n",
    "                                       target_test=target_test, \n",
    "                                       min_NPV=min_NPV,\n",
    "                                       min_NPV_Models=min_NPV_Models,\n",
    "                                       standardization=std_method, \n",
    "                                       imputation=imputation_method, \n",
    "                                       do_nan_masking=do_nan_masking,\n",
    "                                       do_nan_masking_univ=do_nan_masking_univ,\n",
    "                                       nan_masking=nan_masking, \n",
    "                                       do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                       groups=groups,\n",
    "                                       do_preprocessing=do_preprocessing, \n",
    "                                       fix_outliers=fix_outliers,\n",
    "                                       do_imputation=do_imputation)\n",
    "\n",
    "        for model in Results_N_av.keys():\n",
    "            for set_name in Results_N_av[model].keys():\n",
    "                Ground_Truth_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train_Labels'])\n",
    "                Ground_Truth_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test_Labels'])\n",
    "                Results_N_av[model][set_name]['Train_value'].append(Results[model][set_name]['Train_value'])\n",
    "                Results_N_av[model][set_name]['Test_value'].append(Results[model][set_name]['Test_value'])\n",
    "                Results_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train'])\n",
    "                Results_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test'])\n",
    "                \n",
    "                if model=='LR' and 'minNPV' in set_name:\n",
    "                    Results_N_av[model][set_name]['Cutoff'].append(Results[model][set_name]['Cutoff'])\n",
    "\n",
    "                if model=='LR' and '_minNPV' not in set_name:\n",
    "                    Results_N_av[model][set_name]['Weights'].append(Results[model][set_name]['Weights'])\n",
    "                    Results_N_av[model][set_name]['Bias'].append(Results[model][set_name]['Bias'])\n",
    "                    Results_N_av[model][set_name]['C'].append(Results[model][set_name]['C'])\n",
    "                    Results_N_av[model][set_name]['Std_Parameters'].append(Results[model][set_name]['Std_Parameters'])\n",
    "                        \n",
    "                if do_statistic_misclassified and '_minNPV' not in set_name:\n",
    "                    ID_test = Results[model][set_name]['ID_test']\n",
    "                    labels_test = Results[model][set_name]['Test_Labels']\n",
    "                    pred_test = Results[model][set_name]['Test']\n",
    "                    \n",
    "                    mask_positives = labels_test==1\n",
    "                    ID_positives = ID_test[mask_positives]\n",
    "                    Positives_N_av[model].loc[ID_positives, [set_name]] +=1\n",
    "                    mask_false_negatives = mask_positives & (pred_test==0)\n",
    "                    ID_false_negatives = ID_test[mask_false_negatives]\n",
    "                    False_negatives_N_av[model].loc[ID_false_negatives, [set_name]] +=1\n",
    "\n",
    "                    mask_negatives = labels_test==0\n",
    "                    ID_negatives = ID_test[mask_negatives]\n",
    "                    Negatives_N_av[model].loc[ID_negatives, [set_name]] +=1\n",
    "                    mask_false_positives = mask_negatives & (pred_test==1)\n",
    "                    ID_false_positives = ID_test[mask_false_positives]\n",
    "                    False_positives_N_av[model].loc[ID_false_positives, [set_name]] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    coefficients_dict = {}\n",
    "\n",
    "    if 'LR' in Results_N_av.keys():\n",
    "        for set_name in Results_N_av['LR'].keys():\n",
    "            if '_minNPV' not in set_name:\n",
    "                weights = np.array(Results_N_av['LR'][set_name]['Weights']).reshape(-1,)\n",
    "                bias = np.array(Results_N_av['LR'][set_name]['Bias']).reshape(-1,)\n",
    "                C_reg = np.array(Results_N_av['LR'][set_name]['C']).reshape(-1,)\n",
    "                Std_Parameters = np.array(Results_N_av['LR'][set_name]['Std_Parameters']).reshape(-1,)\n",
    "\n",
    "                coefficients_dict[set_name] = weights\n",
    "                \n",
    "coefficients_df = pd.DataFrame.from_dict(coefficients_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    Performances = {}\n",
    "\n",
    "    Scores = {'roc_auc_score': roc_auc_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'NPV': precision_score,\n",
    "              'specificity': recall_score,\n",
    "              'accuracy': accuracy_score}\n",
    "\n",
    "    for model in Results_N_av.keys():\n",
    "        Performances[model] = {}\n",
    "\n",
    "        for set_name in Results_N_av[model].keys():\n",
    "            #print(model, set_name)\n",
    "            Performances[model][set_name] = {}\n",
    "            print(model, set_name)\n",
    "\n",
    "            for score in Scores.keys():\n",
    "                Performances[model][set_name][score] = {'Train': [], 'Test': []}\n",
    "                sc = Scores[score]\n",
    "                for sample in range(N_av):\n",
    "                    y_pred_train = Results_N_av[model][set_name]['Train'][sample]\n",
    "                    y_value_train = Results_N_av[model][set_name]['Train_value'][sample]\n",
    "                    y_train = Ground_Truth_N_av[model][set_name]['Train'][sample]\n",
    "                    y_pred_test = Results_N_av[model][set_name]['Test'][sample]\n",
    "                    y_value_test = Results_N_av[model][set_name]['Test_value'][sample]\n",
    "                    y_test = Ground_Truth_N_av[model][set_name]['Test'][sample]\n",
    "\n",
    "                    y_pred_train_1 = 1 - y_pred_train\n",
    "                    y_train_1 = 1 - y_train\n",
    "                    y_pred_test_1 = 1 - y_pred_test\n",
    "                    y_test_1 = 1 - y_test\n",
    "\n",
    "                    if score in ['NPV', 'specificity']:\n",
    "                        if sum(pd.notnull(y_pred_train_1))>1:\n",
    "                            score_val_train = sc(y_train_1, y_pred_train_1)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test_1, y_pred_test_1)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "                    elif score=='roc_auc_score':\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_value_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test))>1:\n",
    "                            score_val_test = sc(y_test, y_value_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "                    else:\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_pred_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test, y_pred_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    Performances[model][set_name][score]['Train'].append(score_val_train)\n",
    "                    Performances[model][set_name][score]['Test'].append(score_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LRTE/uL'\n",
    "score_name = 'roc_auc_score' # 'specificity', 'NPV', 'roc_auc_score'\n",
    "print(Performances['LR'][model_name].keys())\n",
    "np.mean(Performances['LR'][model_name][score_name]['Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    ## Mean performances\n",
    "    Performances_mean_err_train = {}\n",
    "    Performances_mean_err_train_0 = {}\n",
    "\n",
    "    test_or_train = 'Train'\n",
    "    minNPV_notnull_prop = 0.9\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_err_train[score] = {'Mean':[], 'Err':[]}\n",
    "        Performances_mean_err_train_0[score] = {'Mean':[], 'Err':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                if 'minNPV' in set_name:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_train_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_train_0[score]['Err'].append(score_err)\n",
    "\n",
    "                else:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_train[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_train[score]['Err'].append(score_err)\n",
    "\n",
    "\n",
    "\n",
    "    Performances_mean_err_test = {}\n",
    "    Performances_mean_err_test_0 = {}\n",
    "\n",
    "    test_or_train = 'Test'\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_err_test[score] = {'Mean':[], 'Err':[]}\n",
    "        Performances_mean_err_test_0[score] = {'Mean':[], 'Err':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                if 'minNPV' in set_name:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>minNPV_notnull_prop*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_test_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_test_0[score]['Err'].append(score_err)\n",
    "\n",
    "                else:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>minNPV_notnull_prop*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_test[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_test[score]['Err'].append(score_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [name for name in Results_N_av[model].keys() if '_minNPV' not in name]\n",
    "pd.DataFrame(Performances_mean_err_test_0['specificity']['Mean'], index=models_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average cutoff thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    cutoff_mean_err_train = {'Mean':[], 'Err':[]}\n",
    "\n",
    "    test_or_train = 'Train'\n",
    "\n",
    "    for model in Results_N_av.keys():\n",
    "        for set_name in Results_N_av[model].keys():\n",
    "\n",
    "\n",
    "            if 'minNPV' in set_name:\n",
    "                v = np.array(Results_N_av[model][set_name]['Cutoff'])\n",
    "                mask = pd.notnull(v)\n",
    "                if sum(mask)>0.5*len(mask):\n",
    "                    v = v[pd.notnull(v)]\n",
    "                    ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                    cutoff_mean = np.mean(ci)\n",
    "                    cutoff_err = (ci[1] - ci[0])/2.\n",
    "                else:\n",
    "                    cutoff_mean = 0\n",
    "                    cutoff_err = 0\n",
    "\n",
    "                cutoff_mean_err_train['Mean'].append(cutoff_mean)\n",
    "                cutoff_mean_err_train['Err'].append(cutoff_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    # Create directory\n",
    "    try:\n",
    "        os.mkdir(path_results+foldername_univ)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory failed\")\n",
    "    else:\n",
    "        print (\"Successfully created the directory\")\n",
    "\n",
    "    path_export = path_results+foldername_univ\n",
    "\n",
    "    # Export \n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, err) #\n",
    "    my_tuples = [(score, val) for score in Scores for val in ['Mean', 'Err']]\n",
    "    new_columns = pd.MultiIndex.from_tuples(my_tuples)\n",
    "    index = []\n",
    "    for model in models_dict.keys():\n",
    "        index.extend([model+': '+name for name in models_dict[model].keys()])\n",
    "    n_models = len(index)\n",
    "    values = np.array([np.array(Performances_mean_err_train[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_train.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_err_train_0[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_train.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, err) #\n",
    "    values = np.array([np.array(Performances_mean_err_test[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_test.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_err_test_0[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_test.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "    # Export cutoffs\n",
    "    if min_NPV_Models:\n",
    "        models_list = [name for name in Results_N_av[model].keys() if '_minNPV' not in name]\n",
    "        df_cutoff = pd.DataFrame(cutoff_mean_err_train, index=models_list)\n",
    "        filename_0 = 'cutoffs_minNPV_test.xlsx'\n",
    "        df_cutoff.to_excel(path_export+filename_0, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    # Export models description\n",
    "    models_description = {}\n",
    "    for model in models_dict.keys():\n",
    "        for name in models_dict[model].keys():\n",
    "            description = ' # '.join(models_dict[model][name])\n",
    "            models_description[model+'#'+name] = description\n",
    "\n",
    "    filename = 'models_description.xlsx'\n",
    "    pd.Series(models_description).to_excel(path_export+filename)\n",
    "\n",
    "\n",
    "    # Export models parameters\n",
    "    '''\n",
    "    for set_name in parameters_dict['LR'].keys():\n",
    "        df = parameters_dict['LR'][set_name]\n",
    "        filename = 'Parameters_model#%s_name#%s.xlsx' % ('LR', set_name)\n",
    "        df.to_excel(path_export+filename)\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Export models std parameters\n",
    "    '''\n",
    "    for set_name in std_parameters_dict['LR'].keys():\n",
    "        df = std_parameters_dict['LR'][set_name]\n",
    "        filename = 'StdParameters_model#%s_name#%s.xlsx' % ('LR', set_name)\n",
    "        df.to_excel(path_export+filename)\n",
    "    '''\n",
    "\n",
    "        \n",
    "    # Export misclassifications\n",
    "    '''\n",
    "    for model in models_dict['LR'].keys():\n",
    "        df_negatives = Negatives_N_av[model]\n",
    "        filename = 'Negatives_model#%s.xlsx' % model\n",
    "        df_negatives.to_excel(path_export+filename)\n",
    "        df_false_positives = False_positives_N_av[model]\n",
    "        filename = 'FalsePositives_model#%s.xlsx' % model\n",
    "        df_false_positives.to_excel(path_export+filename)\n",
    "        #\n",
    "        df_positives = Positives_N_av[model]\n",
    "        filename = 'Positives_model#%s.xlsx' % model\n",
    "        df_positives.to_excel(path_export+filename)\n",
    "        df_false_negatives = False_negatives_N_av[model]\n",
    "        filename = 'FalseNegatives_model#%s.xlsx' % model\n",
    "        df_false_negatives.to_excel(path_export+filename)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "green = '#7FB285'\n",
    "violet = '#A888BF'\n",
    "light_red_purp = '#FF616D'\n",
    "red_purp = '#F73B5C'\n",
    "light_blue = '#3C8DAD'\n",
    "dark_blue = '#125D98'\n",
    "orange = '#F5A962'\n",
    "lavander = '#D5C6E0'\n",
    "dark_liver = '#56494C'\n",
    "electricblue = '#7DF9FF'\n",
    "grey_1 = '#AAAAAA'\n",
    "grey_2 = '#DDDDDD'\n",
    "\n",
    "# Color scheme\n",
    "color_0 = dark_blue\n",
    "color_1 = red_purp\n",
    "color_control = green\n",
    "\n",
    "# Parameters\n",
    "fontsize = 9\n",
    "ylabelsize = 7\n",
    "xlabelsize = 7 \n",
    "tex = True\n",
    "axes_lines_w = 0.5\n",
    "lines_w = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'LRTE/uL' # 'LRTE/uL', 'IP10', 'PROADM' \n",
    "\n",
    "Data_train = DataInpatients.copy()\n",
    "Data_test = DataOutpatients.copy() # Data_Control\n",
    "if model_name not in immunecells_set:\n",
    "    Data_test = DataInpatients.copy() \n",
    "    \n",
    "print(Data_train.shape, Data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = None\n",
    "columns = models_dict['LR'][model_name]\n",
    "fix_outliers = False\n",
    "do_imputation = True\n",
    "groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "#columns = list(np.random.permutation(columns))\n",
    "\n",
    "results, MLR, prep_data = CL.LR_model_results(Data=Data_train,\n",
    "                                              Data_test=Data_test,\n",
    "                                              features=columns, \n",
    "                                              set_name=model_name, \n",
    "                                              target_train=target_train, \n",
    "                                              target_test=target_test, \n",
    "                                              test_size=test_size,\n",
    "                                              hyperp_dict={}, \n",
    "                                              do_preprocessing=do_preprocessing, \n",
    "                                              fix_outliers=fix_outliers, \n",
    "                                              do_imputation=do_imputation, \n",
    "                                              imputation=imputation_method, \n",
    "                                              pca_var_threshold=pc_var_th, \n",
    "                                              standardization=std_method, \n",
    "                                              min_NPV_Models=min_NPV_Models,\n",
    "                                              min_NPV=min_NPV, \n",
    "                                              groups=groups, \n",
    "                                              do_nan_masking=do_nan_masking, \n",
    "                                              do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                              do_nan_masking_univ=do_nan_masking_univ, \n",
    "                                              nan_masking=nan_masking,\n",
    "                                              return_model=True, \n",
    "                                              return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data histogram\n",
    "Data_X = prep_data['Train']\n",
    "y_train = results[model_name]['Train_Labels']\n",
    "y_pred = results[model_name]['Train']\n",
    "value_pred = results[model_name]['Train_value']\n",
    "std = StandardScaler()\n",
    "value_pred = std.fit_transform(value_pred.reshape(-1, 1)).reshape(-1,)\n",
    "x_test = results[model_name]['Test_value']\n",
    "x_test = std.transform(x_test.reshape(-1, 1)).reshape(-1,)\n",
    "mask_0 = y_train==0\n",
    "mask_1 = y_train==1\n",
    "x0 = value_pred[mask_0]\n",
    "x1 = value_pred[mask_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnification = 0.85\n",
    "ratio = 2.4/3.7\n",
    "CF.SetPlotParams(magnification=magnification, ratio=ratio, \n",
    "                        fontsize=fontsize, ylabelsize=ylabelsize, xlabelsize=xlabelsize, \n",
    "                        tex=tex, axes_lines_w=axes_lines_w, lines_w=lines_w)\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "mec = 'white'\n",
    "ms = 9\n",
    "mew = 0.15\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "y_target = y_train\n",
    "th = CL.best_threshold_class0(y_pred=y_pred, value_pred=value_pred, y_target=y_train, min_NPV=0.96, fixed_threshold=False)\n",
    "y_pred = np.zeros_like(y_target)\n",
    "y_pred[value_pred>=th] = 1\n",
    "y_pred_1 = 1 - y_pred\n",
    "y_target_1 = 1 - y_target\n",
    "npv_model = precision_score(y_target_1, y_pred_1)\n",
    "spe_model = recall_score(y_target_1, y_pred_1)\n",
    "print('NPV: %.2f'%npv_model, '\\nSpec.: %.2f'%spe_model)\n",
    "\n",
    "x_min = min([np.nanmin(x0), np.nanmin(x1), np.nanmin(x_test)])\n",
    "x_max = max([np.nanmax(x0), np.nanmax(x1), np.nanmax(x_test)])\n",
    "if model_name in immunecells_set:\n",
    "    x_control = x_test\n",
    "    print('control: %d' % (round(100*np.sum(x_control<th)/len(x_control))))\n",
    "\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 14)\n",
    "\n",
    "color = color_0\n",
    "density = False\n",
    "ax.hist(x0, bins,\n",
    "        alpha=0.15,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density)\n",
    "ax.hist(x0, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None', \n",
    "        label='no event')\n",
    "\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 13)\n",
    "\n",
    "if model_name in immunecells_set:\n",
    "    color = color_control\n",
    "    ax.hist(x_control, bins,\n",
    "            alpha=0.2,\n",
    "            histtype='stepfilled',\n",
    "            color=color,\n",
    "            density=density,\n",
    "            label='control')\n",
    "    ax.hist(x_control, bins,\n",
    "            alpha=1.,\n",
    "            histtype='step',\n",
    "            edgecolor=color,\n",
    "            density=density,\n",
    "            facecolor='None')\n",
    "\n",
    "color = color_1\n",
    "ax.hist(x1, bins,\n",
    "        alpha=0.2,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density,\n",
    "        label='event')\n",
    "ax.hist(x1, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None')\n",
    "\n",
    "color_boundary = CF.lighten_color(grey_1, 0.6)\n",
    "#ax.axvline(th, ls='-', lw=0.9, color=electricblue)\n",
    "ax.axvline(th, ls='--', lw=0.7, color='black') \n",
    "\n",
    "ax.title.set_text('npv=%.2f $\\,\\,\\,$ spe.=%.2f'%(npv_model, spe_model))\n",
    "ax.set_xlabel('%s score' % (CF.change_names_2([model_name])[0]))\n",
    "ax.set_ylabel('n. patients')\n",
    "if density:\n",
    "    ax.set_ylabel('patients (density)')\n",
    "\n",
    "\n",
    "if model_name=='LRTE/uL':\n",
    "        ax.set_yticks([0., 45., 90])\n",
    "        ax.set_ylim([0, 90])\n",
    "if model_name=='PROADM':\n",
    "        ax.set_yticks([0., 40., 80.])\n",
    "        ax.set_ylim([0, 80])\n",
    "if model_name=='IP10':\n",
    "        ax.set_yticks([0., 20., 40.])\n",
    "        ax.set_ylim([0, 40])\n",
    "\n",
    "\n",
    "leg = ax.legend(loc='center left', bbox_to_anchor=(0.98, 0.5), frameon=False) #legend(loc=0, frameon=False)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    lh.set_edgecolor('white')\n",
    "\n",
    "filename = 'HistogramLR#%s' % (CF.change_names_2([model_name])[0])\n",
    "filename = filename + exp_description + exp_univ_description + '.pdf'\n",
    "saving_str = path_figures + filename\n",
    "#plt.savefig(saving_str, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_6_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
