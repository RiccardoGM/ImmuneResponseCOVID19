{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['PATH'] = os.environ['PATH'] + ':/Library/TeX/texbin'\n",
    "sys.path.append('/Users/riccardo/Documents/GitHub/COVID19Classification/')\n",
    "\n",
    "from Modules import Parameters, DataPreprocessing as DP, Classification as CL, CustomFunctions as CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multivariate sets\n",
    "FC_set = Parameters.FC_set\n",
    "Dem_set = Parameters.Dem_set\n",
    "CK_set = Parameters.CK_set\n",
    "BM_set =  Parameters.BM_set\n",
    "\n",
    "## Targets\n",
    "target_train = Parameters.train_target\n",
    "target_test = Parameters.test_target\n",
    "\n",
    "## Age\n",
    "lower_bound = Parameters.age_min\n",
    "upper_bound = Parameters.age_max\n",
    "\n",
    "## Delta onset\n",
    "lower_bound_donset = Parameters.donset_min\n",
    "upper_bound_donset = Parameters.donset_max\n",
    "\n",
    "## Minimum NPV\n",
    "min_NPV_Models = Parameters.min_NPV_Models\n",
    "min_NPV = Parameters.min_NPV\n",
    "\n",
    "## Correlation threshod\n",
    "corr_th_univ = Parameters.corr_th_univ\n",
    "\n",
    "## Min % no nans per column\n",
    "perc_nonans = Parameters.perc_nonans\n",
    "perc_nonans_univ = Parameters.perc_nonans_univ\n",
    "\n",
    "## Nan masking row-wise\n",
    "do_nan_masking = Parameters.do_nan_masking\n",
    "do_nan_masking_univ = Parameters.do_nan_masking_univ\n",
    "nan_masking = Parameters.nan_masking\n",
    "do_nan_masking_groupwise = Parameters.do_nan_masking_groupwise\n",
    "\n",
    "## Reference time\n",
    "ref_time = Parameters.ref_time\n",
    "\n",
    "## N samples for average\n",
    "N_av = Parameters.N_av\n",
    "\n",
    "## Imputation\n",
    "imputation_method = Parameters.imputation_method\n",
    "imputation_method_univ = Parameters.imputation_method_univ\n",
    "\n",
    "## Standardization\n",
    "std_method = Parameters.std_method\n",
    "std_cat_variables = Parameters.std_cat_variables\n",
    "\n",
    "## PCA % var. threshold\n",
    "pc_var_th = Parameters.pca_var_threshold\n",
    "\n",
    "## Preprocessing\n",
    "do_preprocessing = Parameters.do_preprocessing_multiv\n",
    "\n",
    "## Train-test\n",
    "test_size = Parameters.test_size\n",
    "ignore_sex = Parameters.ignore_sex\n",
    "\n",
    "## Plot\n",
    "plot_minNPV_models = Parameters.plot_minNPV_models\n",
    "\n",
    "## Feature selection\n",
    "use_manual_selection = Parameters.use_manual_selection\n",
    "\n",
    "## Paths\n",
    "path_datasets = Parameters.path_datasets\n",
    "path_results = Parameters.path_results\n",
    "path_figures = Parameters.path_figures\n",
    "path_setsdescription = Parameters.path_setsdescription\n",
    "exp_description = Parameters.exp_description\n",
    "exp_univ_description = Parameters.exp_univ_description\n",
    "exp_multiv_description = Parameters.exp_multiv_description\n",
    "foldername_univ = Parameters.foldername_univ\n",
    "foldername_multiv = Parameters.foldername_multiv\n",
    "\n",
    "## Dataset\n",
    "use_CCIMasked_dataset = Parameters.use_CCIMasked_dataset\n",
    "\n",
    "## Regulariser\n",
    "find_regulariser_before_average = Parameters.find_regulariser_before_average\n",
    "\n",
    "## Run experiments\n",
    "run_experiments = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Age min:', lower_bound)\n",
    "print('Age max:', upper_bound)\n",
    "print('Donset min:', lower_bound_donset)\n",
    "print('Donset max:', upper_bound_donset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataInpatients, DataOutpatients = DP.data_preprocessing()\n",
    "\n",
    "# Mask data by target\n",
    "mask = pd.notnull(DataInpatients[target_train].values) & pd.notnull(DataInpatients[target_test].values)\n",
    "DataInpatients = DataInpatients.loc[mask,:]\n",
    "\n",
    "DataInpatients.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "\n",
    "## LR models\n",
    "models_dict['LR'] = {}\n",
    "models_dict['LR']['FC'] = FC_set\n",
    "models_dict['LR']['FC+Dem'] = FC_set + Dem_set\n",
    "models_dict['LR']['CK'] = CK_set\n",
    "models_dict['LR']['CK+Dem'] = CK_set + Dem_set\n",
    "models_dict['LR']['BM'] = BM_set\n",
    "models_dict['LR']['BM+Dem'] = BM_set + Dem_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_masking_val = 0.5\n",
    "do_nan_masking_groupwise_val = True\n",
    "groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "\n",
    "CCI_th = 2\n",
    "if lower_bound>60:\n",
    "    CCI_th = 4\n",
    "dict_stats = { 'N': [], 'NANs': [], 'sex': [], 'age': [], 'd. onset': [], 'CCI<%d'%CCI_th: [], 'OTI+death': []}\n",
    "\n",
    "for model_name in models_dict['LR'].keys():\n",
    "    print(model_name)\n",
    "    \n",
    "    # Mask data\n",
    "    Data = DataInpatients.copy()\n",
    "    _, row_mask = CL.nan_masking_fc(Data, models_dict['LR'][model_name], \n",
    "                                    nan_masking=nan_masking_val, \n",
    "                                    do_nan_masking_groupwise=do_nan_masking_groupwise_val, \n",
    "                                    groups=groups, \n",
    "                                    return_row_mask=True)\n",
    "\n",
    "    # N. patients\n",
    "    n_patients = sum(row_mask)\n",
    "    val_str = '%d' % n_patients\n",
    "    #print('N: '+val_str)\n",
    "    dict_stats['N'].append(val_str)\n",
    "\n",
    "    # NANs\n",
    "    n = DataInpatients.loc[row_mask, models_dict['LR'][model_name]].size\n",
    "    n_nan = sum(DataInpatients.loc[row_mask, models_dict['LR'][model_name]].isna().values.flatten())\n",
    "    ratio = n_nan/n*100\n",
    "    val_str = '%.1f%%' % ratio\n",
    "    #print('NANs: '+val_str)\n",
    "    dict_stats['NANs'].append(val_str)\n",
    "    \n",
    "    # Sex\n",
    "    n = sum(DataInpatients.loc[row_mask, 'sex'] == 1)\n",
    "    ratio = n/n_patients*100\n",
    "    val_str = '%d(%.1f%%)' %(n, ratio)\n",
    "    #print('sex: '+val_str)\n",
    "    dict_stats['sex'].append(val_str)\n",
    "\n",
    "    # Age\n",
    "    q1, median, q3 = DataInpatients.loc[row_mask, 'age'].quantile([0.25,0.5,0.75])\n",
    "    val_str = '%d(%d-%d)' % (round(median), round(q1), round(q3))\n",
    "    #print('age: '+val_str)\n",
    "    dict_stats['age'].append(val_str)\n",
    "\n",
    "    # D. onset\n",
    "    q1, median, q3 = DataInpatients.loc[row_mask, 'delta_onset'].quantile([0.25,0.5,0.75])\n",
    "    val_str = '%d(%d-%d)' % (round(median), round(q1), round(q3))\n",
    "    #print('d. onset: '+val_str)\n",
    "    dict_stats['d. onset'].append(val_str)\n",
    "\n",
    "    # CCI\n",
    "    n = sum(DataInpatients.loc[row_mask, 'CCI (charlson comorbidity index)'] < CCI_th)\n",
    "    ratio = n/n_patients*100\n",
    "    val_str = '%d(%.1f%%)' %(n, ratio)\n",
    "    #print('CCI<2: '+val_str)\n",
    "    dict_stats['CCI<%d'%CCI_th].append(val_str)\n",
    "\n",
    "    # Outcome\n",
    "    n = sum(DataInpatients.loc[row_mask, 'IOT+death'] == 1)\n",
    "    ratio = n/n_patients*100\n",
    "    val_str = '%d(%.1f%%)' %(n, ratio)\n",
    "    #print('CCI<2: '+val_str)\n",
    "    dict_stats['OTI+death'].append(val_str)\n",
    "\n",
    "    #print('\\n')\n",
    "\n",
    "df_stat = pd.DataFrame(dict_stats, index=models_dict['LR'].keys())\n",
    "\n",
    "# Export table\n",
    "filename = 'DatasetsDescription'+exp_description+'.xlsx'\n",
    "df_stat.to_excel(path_setsdescription+filename)\n",
    "\n",
    "df_stat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis - multiple splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    Data = DataInpatients.copy()     \n",
    "    \n",
    "    ## Initialize results dictionaries\n",
    "    Results_N_av = {}\n",
    "    Ground_Truth_N_av = {}\n",
    "    for model in models_dict.keys():\n",
    "        Results_N_av[model] = {}\n",
    "        Ground_Truth_N_av[model] = {}\n",
    "        for set_name in models_dict[model].keys():\n",
    "            Results_N_av[model][set_name] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': [], 'Weights':[], 'Bias':[], 'C':[], 'Std_Parameters': []}\n",
    "            Ground_Truth_N_av[model][set_name] = {'Train':[], 'Test':[]}\n",
    "            if min_NPV_Models:\n",
    "                Results_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[], 'Train_value': [], 'Test_value': []}\n",
    "                Ground_Truth_N_av[model][set_name+'_minNPV'] = {'Train':[], 'Test':[]}\n",
    "    \n",
    "    do_statistic_misclassified = True\n",
    "    if do_statistic_misclassified:\n",
    "        positives = Data.loc[Data[target_test].values==1, 'ID'].values\n",
    "        negatives = Data.loc[Data[target_test].values==0, 'ID'].values\n",
    "        Negatives_N_av = {}\n",
    "        False_positives_N_av = {}\n",
    "        Positives_N_av = {}\n",
    "        False_negatives_N_av = {}\n",
    "        for model in models_dict.keys():\n",
    "            model_set_names = models_dict[model].keys()\n",
    "            Negatives_N_av[model] = pd.DataFrame(np.zeros((len(negatives), len(model_set_names))), index=negatives, columns=model_set_names)\n",
    "            False_positives_N_av[model]= pd.DataFrame(np.zeros((len(negatives), len(model_set_names))), index=negatives, columns=model_set_names)\n",
    "            Positives_N_av[model] = pd.DataFrame(np.zeros((len(positives), len(model_set_names))), index=positives, columns=model_set_names)\n",
    "            False_negatives_N_av[model] = pd.DataFrame(np.zeros((len(positives), len(model_set_names))), index=positives, columns=model_set_names)\n",
    "            \n",
    "\n",
    "    ## Sets of variables for nan removal\n",
    "    groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "\n",
    "\n",
    "    ## Run average\n",
    "    fix_outliers = False\n",
    "    do_imputation = True\n",
    "    print('Running average...')\n",
    "    print('Fix outliers: %s' % fix_outliers)\n",
    "    print('Do imputation: %s' % do_imputation)\n",
    "    print('Nan masking groupwise: %s' % do_nan_masking_groupwise)\n",
    "    print('Do preprocessing: %s' % do_preprocessing)\n",
    "    for i_split in range(N_av): \n",
    "        print('%d/%d' %(i_split+1, N_av))\n",
    "        \n",
    "        Results = CL.models_prediction(Data=Data.copy(),\n",
    "                                       test_size=test_size,\n",
    "                                       models_dict=models_dict,\n",
    "                                       target_train=target_train, \n",
    "                                       target_test=target_test, \n",
    "                                       min_NPV=min_NPV,\n",
    "                                       min_NPV_Models=min_NPV_Models,\n",
    "                                       standardization=std_method, \n",
    "                                       imputation=imputation_method, \n",
    "                                       do_nan_masking=do_nan_masking,\n",
    "                                       do_nan_masking_univ=do_nan_masking_univ,\n",
    "                                       nan_masking=nan_masking, \n",
    "                                       do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                       groups=groups,\n",
    "                                       do_preprocessing=do_preprocessing, \n",
    "                                       fix_outliers=fix_outliers,\n",
    "                                       do_imputation=do_imputation, \n",
    "                                       ignore_sex=ignore_sex, \n",
    "                                       std_cat_variables=std_cat_variables)\n",
    "\n",
    "        for model in Results_N_av.keys():\n",
    "            for set_name in Results_N_av[model].keys():\n",
    "                Ground_Truth_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train_Labels'])\n",
    "                Ground_Truth_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test_Labels'])\n",
    "                Results_N_av[model][set_name]['Train_value'].append(Results[model][set_name]['Train_value'])\n",
    "                Results_N_av[model][set_name]['Test_value'].append(Results[model][set_name]['Test_value'])\n",
    "                Results_N_av[model][set_name]['Train'].append(Results[model][set_name]['Train'])\n",
    "                Results_N_av[model][set_name]['Test'].append(Results[model][set_name]['Test'])\n",
    "                \n",
    "                if model=='LR' and '_minNPV' not in set_name:\n",
    "                    Results_N_av[model][set_name]['Weights'].append(Results[model][set_name]['Weights'])\n",
    "                    Results_N_av[model][set_name]['Bias'].append(Results[model][set_name]['Bias'])\n",
    "                    Results_N_av[model][set_name]['C'].append(Results[model][set_name]['C'])\n",
    "                    Results_N_av[model][set_name]['Std_Parameters'].append(Results[model][set_name]['Std_Parameters'])\n",
    "                        \n",
    "                if do_statistic_misclassified and '_minNPV' not in set_name:\n",
    "                    ID_test = Results[model][set_name]['ID_test']\n",
    "                    labels_test = Results[model][set_name]['Test_Labels']\n",
    "                    pred_test = Results[model][set_name]['Test']\n",
    "                    \n",
    "                    mask_positives = labels_test==1\n",
    "                    ID_positives = ID_test[mask_positives]\n",
    "                    Positives_N_av[model].loc[ID_positives, [set_name]] +=1\n",
    "                    mask_false_negatives = mask_positives & (pred_test==0)\n",
    "                    ID_false_negatives = ID_test[mask_false_negatives]\n",
    "                    False_negatives_N_av[model].loc[ID_false_negatives, [set_name]] +=1\n",
    "\n",
    "                    mask_negatives = labels_test==0\n",
    "                    ID_negatives = ID_test[mask_negatives]\n",
    "                    Negatives_N_av[model].loc[ID_negatives, [set_name]] +=1\n",
    "                    mask_false_positives = mask_negatives & (pred_test==1)\n",
    "                    ID_false_positives = ID_test[mask_false_positives]\n",
    "                    False_positives_N_av[model].loc[ID_false_positives, [set_name]] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    parameters_dict = {}\n",
    "    std_parameters_dict = {}\n",
    "\n",
    "    if 'LR' in Results_N_av.keys():\n",
    "        parameters_dict['LR'] = {}\n",
    "        std_parameters_dict['LR'] = {}\n",
    "\n",
    "        for set_name in Results_N_av['LR'].keys():\n",
    "            if '_minNPV' not in set_name:\n",
    "                weights = np.array(Results_N_av['LR'][set_name]['Weights'])\n",
    "                bias = np.array(Results_N_av['LR'][set_name]['Bias'])\n",
    "                C_reg = np.array(Results_N_av['LR'][set_name]['C'])\n",
    "                Std_Parameters = np.array(Results_N_av['LR'][set_name]['Std_Parameters'])\n",
    "\n",
    "                if weights.shape[1]>1:\n",
    "                    column_names = models_dict['LR'][set_name]\n",
    "                    if '#Dis' in set_name:\n",
    "                        new_columns = []\n",
    "                        bool_idx = []\n",
    "                        for col in column_names:\n",
    "                            v = [col1 for col1 in Data.columns if col+'_INT' in col1]\n",
    "                            new_columns += v\n",
    "                            if len(v)>0:\n",
    "                                bool_idx += [False]\n",
    "                            else:\n",
    "                                bool_idx += [True]\n",
    "                        new_columns += list(np.array(column_names)[bool_idx])\n",
    "                        column_names = new_columns\n",
    "                    df = pd.DataFrame(weights, columns=column_names)\n",
    "                    df.insert(loc=0, column='bias', value=bias)\n",
    "                    parameters_dict['LR'][set_name] = df\n",
    "\n",
    "                    column_names = models_dict['LR'][set_name]\n",
    "                    if '#Dis' not in set_name:\n",
    "                        df = pd.DataFrame(Std_Parameters, columns=[col for col in column_names if col!='sex'])\n",
    "                        df.insert(loc=0, column='C', value=C_reg)\n",
    "                    else:\n",
    "                        df = pd.DataFrame(C_reg, columns=['C'])\n",
    "                    std_parameters_dict['LR'][set_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "\n",
    "    Performances = {}\n",
    "\n",
    "    Scores = {'roc_auc_score': roc_auc_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'NPV': precision_score,\n",
    "              'specificity': recall_score,\n",
    "              'accuracy': accuracy_score}\n",
    "\n",
    "    for model in Results_N_av.keys():\n",
    "        Performances[model] = {}\n",
    "\n",
    "        for set_name in Results_N_av[model].keys():\n",
    "            #print(model, set_name)\n",
    "            Performances[model][set_name] = {}\n",
    "            print(model, set_name)\n",
    "\n",
    "            for score in Scores.keys():\n",
    "                Performances[model][set_name][score] = {'Train': [], 'Test': []}\n",
    "                sc = Scores[score]\n",
    "                for sample in range(N_av):\n",
    "                    y_pred_train = Results_N_av[model][set_name]['Train'][sample]\n",
    "                    y_value_train = Results_N_av[model][set_name]['Train_value'][sample]\n",
    "                    y_train = Ground_Truth_N_av[model][set_name]['Train'][sample]\n",
    "                    y_pred_test = Results_N_av[model][set_name]['Test'][sample]\n",
    "                    y_value_test = Results_N_av[model][set_name]['Test_value'][sample]\n",
    "                    y_test = Ground_Truth_N_av[model][set_name]['Test'][sample]\n",
    "\n",
    "                    y_pred_train_1 = 1 - y_pred_train\n",
    "                    y_train_1 = 1 - y_train\n",
    "                    y_pred_test_1 = 1 - y_pred_test\n",
    "                    y_test_1 = 1 - y_test\n",
    "\n",
    "                    if score in ['NPV', 'specificity']:\n",
    "                        if sum(pd.notnull(y_pred_train_1))>1:\n",
    "                            score_val_train = sc(y_train_1, y_pred_train_1)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test_1, y_pred_test_1)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "                    elif score=='roc_auc_score':\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_value_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test))>1:\n",
    "                            score_val_test = sc(y_test, y_value_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "                    else:\n",
    "                        if sum(pd.notnull(y_pred_train))>1:\n",
    "                            score_val_train = sc(y_train, y_pred_train)\n",
    "                        else:\n",
    "                            score_val_train = np.nan\n",
    "\n",
    "                        if sum(pd.notnull(y_pred_test_1))>1:\n",
    "                            score_val_test = sc(y_test, y_pred_test)\n",
    "                        else:\n",
    "                            score_val_test = np.nan\n",
    "\n",
    "                    Performances[model][set_name][score]['Train'].append(score_val_train)\n",
    "                    Performances[model][set_name][score]['Test'].append(score_val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    ## Mean performances\n",
    "    Performances_mean_err_train = {}\n",
    "    Performances_mean_err_train_0 = {}\n",
    "\n",
    "    test_or_train = 'Train'\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_err_train[score] = {'Mean':[], 'Err':[]}\n",
    "        Performances_mean_err_train_0[score] = {'Mean':[], 'Err':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                if 'minNPV' in set_name:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_train_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_train_0[score]['Err'].append(score_err)\n",
    "\n",
    "                else:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_train[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_train[score]['Err'].append(score_err)\n",
    "\n",
    "\n",
    "\n",
    "    Performances_mean_err_test = {}\n",
    "    Performances_mean_err_test_0 = {}\n",
    "\n",
    "    test_or_train = 'Test'\n",
    "\n",
    "    for score in Scores:\n",
    "        Performances_mean_err_test[score] = {'Mean':[], 'Err':[]}\n",
    "        Performances_mean_err_test_0[score] = {'Mean':[], 'Err':[]}\n",
    "\n",
    "    for model in Performances.keys():\n",
    "        for set_name in Performances[model].keys():\n",
    "            for score in Performances[model][set_name].keys():\n",
    "\n",
    "                if 'minNPV' in set_name:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_test_0[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_test_0[score]['Err'].append(score_err)\n",
    "\n",
    "                else:\n",
    "                    v = np.array(Performances[model][set_name][score][test_or_train])\n",
    "                    mask = pd.notnull(v)\n",
    "                    if sum(mask)>0.5*len(mask):\n",
    "                        v = v[pd.notnull(v)]\n",
    "                        ci = st.t.interval(alpha=0.95, df=len(v)-1, loc=np.mean(v), scale=st.sem(v))\n",
    "                        score_mean = np.mean(ci)\n",
    "                        score_err = (ci[1] - ci[0])/2.\n",
    "                    else:\n",
    "                        score_mean = 0\n",
    "                        score_err = 0\n",
    "\n",
    "                    Performances_mean_err_test[score]['Mean'].append(score_mean)\n",
    "                    Performances_mean_err_test[score]['Err'].append(score_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    # Create directory\n",
    "    try:\n",
    "        os.mkdir(path_results+foldername_multiv)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory failed\")\n",
    "    else:\n",
    "        print (\"Successfully created the directory\")\n",
    "\n",
    "    path_export = path_results+foldername_multiv\n",
    "\n",
    "    # Export \n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, err) #\n",
    "    my_tuples = [(score, val) for score in Scores for val in ['Mean', 'Err']]\n",
    "    new_columns = pd.MultiIndex.from_tuples(my_tuples)\n",
    "    index = []\n",
    "    for model in models_dict.keys():\n",
    "        index.extend([model+': '+name for name in models_dict[model].keys()])\n",
    "    n_models = len(index)\n",
    "    values = np.array([np.array(Performances_mean_err_train[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_train.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_err_train_0[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_train.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')\n",
    "\n",
    "\n",
    "    # Tuples here are: (name_score, mean) (name_score, err) #\n",
    "    values = np.array([np.array(Performances_mean_err_test[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "    df_results = pd.DataFrame(values, columns=new_columns, index=index)\n",
    "    filename = 'performances_test.xlsx'\n",
    "    df_results.to_excel(path_export+filename, engine='xlsxwriter')\n",
    "\n",
    "    if min_NPV_Models:\n",
    "        values_0 = np.array([np.array(Performances_mean_err_test_0[score][val]) for score in Scores for val in ['Mean', 'Err']]).transpose()\n",
    "        df_results_0 = pd.DataFrame(values_0, columns=new_columns, index=index)\n",
    "        filename_0 = 'performances_minNPV_test.xlsx'\n",
    "        df_results_0.to_excel(path_export+filename_0, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_experiments:\n",
    "    # Export models description\n",
    "    models_description = {}\n",
    "    for model in models_dict.keys():\n",
    "        for name in models_dict[model].keys():\n",
    "            description = ' # '.join(models_dict[model][name])\n",
    "            models_description[model+'#'+name] = description\n",
    "\n",
    "    filename = 'models_description.xlsx'\n",
    "    pd.Series(models_description).to_excel(path_export+filename)\n",
    "\n",
    "\n",
    "    # Export models parameters\n",
    "    for set_name in parameters_dict['LR'].keys():\n",
    "        df = parameters_dict['LR'][set_name]\n",
    "        filename = 'Parameters_model#%s_name#%s.xlsx' % ('LR', set_name)\n",
    "        df.to_excel(path_export+filename)\n",
    "\n",
    "\n",
    "    # Export models std parameters\n",
    "    for set_name in std_parameters_dict['LR'].keys():\n",
    "        df = std_parameters_dict['LR'][set_name]\n",
    "        filename = 'StdParameters_model#%s_name#%s.xlsx' % ('LR', set_name)\n",
    "        df.to_excel(path_export+filename)\n",
    "        \n",
    "        \n",
    "    # Export misclassifications\n",
    "    '''\n",
    "    for model in models_dict['LR'].keys():\n",
    "        df_negatives = Negatives_N_av[model]\n",
    "        filename = 'Negatives_model#%s.xlsx' % model\n",
    "        df_negatives.to_excel(path_export+filename)\n",
    "        df_false_positives = False_positives_N_av[model]\n",
    "        filename = 'FalsePositives_model#%s.xlsx' % model\n",
    "        df_false_positives.to_excel(path_export+filename)\n",
    "        #\n",
    "        df_positives = Positives_N_av[model]\n",
    "        filename = 'Positives_model#%s.xlsx' % model\n",
    "        df_positives.to_excel(path_export+filename)\n",
    "        df_false_negatives = False_negatives_N_av[model]\n",
    "        filename = 'FalseNegatives_model#%s.xlsx' % model\n",
    "        df_false_negatives.to_excel(path_export+filename)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "green = '#7FB285'\n",
    "violet = '#A888BF'\n",
    "light_red_purp = '#FF616D'\n",
    "red_purp = '#F73B5C'\n",
    "light_blue = '#3C8DAD'\n",
    "dark_blue = '#125D98'\n",
    "orange = '#F5A962'\n",
    "lavander = '#D5C6E0'\n",
    "dark_liver = '#56494C'\n",
    "electricblue = '#7DF9FF'\n",
    "grey_1 = '#AAAAAA'\n",
    "grey_2 = '#DDDDDD'\n",
    "\n",
    "# Color scheme\n",
    "color_0 = dark_blue\n",
    "color_1 = red_purp\n",
    "color_control = green\n",
    "\n",
    "# Parameters\n",
    "fontsize = 9\n",
    "ylabelsize = 7\n",
    "xlabelsize = 7 \n",
    "tex = True\n",
    "axes_lines_w = 0.5\n",
    "lines_w = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2D Plot - test outpatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CK+Dem' \n",
    "\n",
    "Data_train = DataInpatients.copy()\n",
    "Data_test = DataOutpatients.copy() # Data_Control\n",
    "if 'FC' not in model_name:\n",
    "    Data_test = DataInpatients.copy() \n",
    "    \n",
    "print(Data_train.shape, Data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = None\n",
    "columns = models_dict['LR'][model_name]\n",
    "fix_outliers = False\n",
    "do_imputation = True\n",
    "groups = [FC_set, CK_set, Dem_set, BM_set]\n",
    "#columns = list(np.random.permutation(columns))\n",
    "\n",
    "results, MLR, prep_data = CL.LR_model_results(Data=Data_train,\n",
    "                                              Data_test=Data_test,\n",
    "                                              features=columns, \n",
    "                                              set_name=model_name, \n",
    "                                              target_train=target_train, \n",
    "                                              target_test=target_test, \n",
    "                                              test_size=test_size,\n",
    "                                              hyperp_dict={}, \n",
    "                                              do_preprocessing=do_preprocessing, \n",
    "                                              fix_outliers=fix_outliers, \n",
    "                                              do_imputation=do_imputation, \n",
    "                                              imputation=imputation_method, \n",
    "                                              pca_var_threshold=pc_var_th, \n",
    "                                              standardization=std_method, \n",
    "                                              min_NPV_Models=min_NPV_Models,\n",
    "                                              min_NPV=min_NPV, \n",
    "                                              groups=groups, \n",
    "                                              do_nan_masking=do_nan_masking, \n",
    "                                              do_nan_masking_groupwise=do_nan_masking_groupwise, \n",
    "                                              do_nan_masking_univ=do_nan_masking_univ, \n",
    "                                              nan_masking=nan_masking,\n",
    "                                              return_model=True, \n",
    "                                              return_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for 2D plot\n",
    "Data_X = prep_data['Train']\n",
    "Data_y = results[model_name]['Train_Labels']\n",
    "Data_X_t = prep_data['Test']\n",
    "proj_2D = CF.MLR_axis(MLR, Data_X, Data_test=Data_X_t, use_bias=False) # use_bias=False\n",
    "X2D_train = proj_2D['Train']\n",
    "X2D_test = proj_2D['Test']\n",
    "X1D_mlr_train, X1D_pca_train = X2D_train[:, 0], X2D_train[:, 1]\n",
    "X1D_mlr_test, X1D_pca_test = X2D_test[:, 0], X2D_test[:, 1]\n",
    "X_1, X_2 = X1D_mlr_train, X1D_pca_train\n",
    "X_1_0 = X_1[Data_y==0].copy()\n",
    "X_2_0 = X_2[Data_y==0].copy()\n",
    "X_1_1 = X_1[Data_y==1].copy()\n",
    "X_2_1 = X_2[Data_y==1].copy()\n",
    "X_1_t, X_2_t = X1D_mlr_test.copy(), X1D_pca_test.copy()\n",
    "\n",
    "## Define grid\n",
    "h = .05 # .02  # step size in the mesh\n",
    "pref_lims = 0.2\n",
    "offset_lims = 0.\n",
    "x_1_min, x_1_max = X_1.min() - pref_lims*(X_1.max()-X_1.min()), X_1.max() + pref_lims*(X_1.max()-X_1.min())\n",
    "x_2_min, x_2_max = X_2.min() - pref_lims*(X_2.max()-X_2.min()), X_2.max() + pref_lims*(X_2.max()-X_2.min())\n",
    "X_1_t, X_2_t = X1D_mlr_test, X1D_pca_test\n",
    "x_1_min_t, x_1_max_t = X_1_t.min() - pref_lims*(X_1_t.max()-X_1_t.min()), X_1_t.max() + pref_lims*(X_1_t.max()-X_1_t.min())\n",
    "x_2_min_t, x_2_max_t = X_2_t.min() - pref_lims*(X_2_t.max()-X_2_t.min()), X_2_t.max() + pref_lims*(X_2_t.max()-X_2_t.min())\n",
    "if x_1_min_t<x_1_min:\n",
    "    x_1_min = x_1_min_t\n",
    "if x_2_min_t<x_2_min:\n",
    "    x_2_min = x_2_min_t\n",
    "if x_1_max_t>x_1_max:\n",
    "    x_1_max = x_1_max_t\n",
    "if x_2_max_t>x_2_max:\n",
    "    x_2_max = x_2_max_t  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Evaluate predictions\n",
    "y_train = Data_y.copy()\n",
    "y_pred = MLR.predict(Data_X) #MLR.fit(Data_X, Data_y).predict(Data_X)\n",
    "y_pred_t = MLR.predict(Data_X_t)\n",
    "value_pred = MLR.decision_function(Data_X)\n",
    "min_NPV_2DPlot = 0.96 # min_NPV\n",
    "threshold_0 = CL.best_threshold_class0(y_pred=y_pred, value_pred=value_pred, y_target=y_train, min_NPV=min_NPV_2DPlot, fixed_threshold=False)\n",
    "\n",
    "## Compute scores\n",
    "f1_train_model = f1_score(y_train, y_pred)\n",
    "rec_train_model = recall_score(y_train, y_pred)\n",
    "pre_train_model = precision_score(y_train, y_pred)\n",
    "NPV_train_model = precision_score(1-y_train, 1-y_pred)\n",
    "spe_train_model = recall_score(1-y_train, 1-y_pred)\n",
    "roc_train_model = roc_auc_score(y_train, value_pred)\n",
    "print('F1:', f1_train_model, 'NPV:', NPV_train_model, 'ROC_AUC:', roc_train_model)\n",
    "\n",
    "y_pred_0 = np.zeros_like(y_pred)\n",
    "y_pred_0[value_pred<=threshold_0] = 0\n",
    "y_pred_0[value_pred>threshold_0] = 1\n",
    "f1_train_minNPVmodel = f1_score(y_train, y_pred_0)\n",
    "rec_train_minNPVmodel = recall_score(y_train, y_pred_0)\n",
    "pre_train_minNPVmodel = precision_score(y_train, y_pred_0)\n",
    "NPV_train_minNPVmodel = precision_score(1-y_train, 1-y_pred_0)\n",
    "spe_train_minNPVmodel = recall_score(1-y_train, 1-y_pred_0)\n",
    "print('F1 (min NPV):', f1_train_minNPVmodel, 'NPV (min NPV):', NPV_train_minNPVmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scatterplot LR score (x) and PC1 (perp to x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters\n",
    "magnification = 2.8\n",
    "plot_heatmap = False\n",
    "plot_test = True\n",
    "plot_immunoparalysis = False\n",
    "alpha_bg = 1\n",
    "mec = 'white'\n",
    "ms = 8\n",
    "mew = 0 #0.05\n",
    "c_0 = dark_blue\n",
    "c_1 = red_purp \n",
    "c_control = green\n",
    "c_immunopar = orange\n",
    "c_background = CF.lighten_color(grey_2, 0.6)\n",
    "n_cols = 3\n",
    "if 'FC' in model_name:\n",
    "    n_cols += 1\n",
    "if plot_immunoparalysis:\n",
    "    n_cols += 1\n",
    "\n",
    "# Set plot style    \n",
    "CF.SetPlotParams(magnification=magnification*n_cols/4., ratio=float(2./(7.9*n_cols/4.)), fontsize=11, \n",
    "                 ylabelsize=9, xlabelsize=9, tex=True, axes_lines_w=0.5, lines_w=0.5, legendmarker=False)\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(nrows=1, ncols=n_cols, sharey=True)\n",
    "\n",
    "\n",
    "## Plot points\n",
    "idx = 0\n",
    "ax[idx].plot(X_1_0, X_2_0, ls=' ', marker='.', color=c_0, ms=ms, mew=mew, mec=mec, alpha=1.)\n",
    "if 'FC' in model_name:\n",
    "    if plot_test:\n",
    "        ax[idx].plot(X_1_t, X_2_t, ls=' ', marker='.', color=c_control, ms=ms, mew=mew, mec=mec, alpha=1.)\n",
    "ax[idx].plot(X_1_1, X_2_1, ls=' ', marker='.', color=c_1, ms=ms, mew=mew, mec=mec, alpha=1.)\n",
    "ax[idx].axvline(threshold_0, color='black', alpha=1., ls='--', dashes=(1.5, 1.5), lw=1.2, \n",
    "                label='NPV: %.2f\\nspe: %.2f' % (NPV_train_minNPVmodel, spe_train_minNPVmodel))\n",
    "ax[idx].axvline(0, color='black', alpha=1., ls='-', lw=1.,\n",
    "                label='NPV: %.2f\\nspe: %.2f' % (NPV_train_model, spe_train_model)) \n",
    "ax[idx].set_xlim(x_1_min, x_1_max)\n",
    "ax[idx].set_xticks(())\n",
    "ax[idx].set_xlabel('x: LR score')\n",
    "ax[idx].set_ylabel('y: PC1 $\\perp$ x')\n",
    "my_legend = ax[idx].legend(markerscale=1., scatterpoints=1, fontsize=11, handlelength=1.5, labelspacing=1.2, frameon=False, \n",
    "               loc='center right', bbox_to_anchor= (-.25, 0.5), ncol=1, borderaxespad=0)\n",
    "my_legend.set_title('AUC=%.2f' % roc_train_model, prop = {'size':11}) #, 'style': 'italic'})\n",
    "ax[idx].title.set_text('outcome: OTI+death')\n",
    "ax[idx].grid(False)\n",
    "\n",
    "\n",
    "\n",
    "idx = idx+1\n",
    "ax[idx].plot(X_1_0, X_2_0, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "if plot_test:\n",
    "    ax[idx].plot(X_1_t, X_2_t, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "ax[idx].plot(X_1_1, X_2_1, ls=' ', marker='.', color=c_1, ms=ms, mew=mew, mec=mec, alpha=0.75, label='Not survived')\n",
    "ax[idx].axvline(threshold_0, color='black', alpha=1., ls='--', dashes=(1.5, 1.5), lw=1.2)\n",
    "ax[idx].axvline(0, color='black', alpha=1., ls='-', lw=1.)\n",
    "ax[idx].set_xlim(x_1_min, x_1_max)\n",
    "ax[idx].set_xticks(())\n",
    "ax[idx].set_xlabel('x: LR score')\n",
    "#ax[idx].yaxis.set_visible(False)\n",
    "#ax[idx].spines['left'].set_visible(False)\n",
    "#ax[idx].legend(loc='lower center', bbox_to_anchor=(0.5, 0.95), frameon=False) #legend(loc=0, frameon=False)\n",
    "ax[idx].title.set_text('event')\n",
    "ax[idx].grid(False)\n",
    "\n",
    "\n",
    "\n",
    "idx = idx+1\n",
    "if plot_test:\n",
    "    ax[idx].plot(X_1_t, X_2_t, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "ax[idx].plot(X_1_1, X_2_1, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "ax[idx].plot(X_1_0, X_2_0, ls=' ', marker='.', color=c_0, ms=ms, mew=mew, mec=mec, alpha=0.75, label='Survived')\n",
    "ax[idx].axvline(threshold_0, color='black', alpha=1., ls='--', dashes=(1.5, 1.5), lw=1.2)\n",
    "ax[idx].axvline(0, color='black', alpha=1., ls='-', lw=1.)\n",
    "ax[idx].set_xlim(x_1_min, x_1_max)\n",
    "ax[idx].set_xticks(())\n",
    "ax[idx].set_xlabel('x: LR score')\n",
    "#ax[idx].yaxis.set_visible(False)\n",
    "#ax[idx].spines['left'].set_visible(False)\n",
    "#ax[idx].legend(loc='lower center', bbox_to_anchor=(0.5, 0.95), frameon=False) #legend(loc=0, frameon=False)\n",
    "ax[idx].title.set_text('no event')\n",
    "ax[idx].grid(False)\n",
    "\n",
    "\n",
    "\n",
    "if 'FC' in model_name:\n",
    "    idx = idx+1\n",
    "    ax[idx].plot(X_1_0, X_2_0, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "    ax[idx].plot(X_1_1, X_2_1, ls=' ', marker='.', color=c_background, ms=ms, mew=mew, mec=mec, alpha=alpha_bg)\n",
    "    if plot_test:\n",
    "        ax[idx].plot(X_1_t, X_2_t, ls=' ', marker='.', color=c_control, ms=ms, mew=mew, mec=mec, alpha=0.75, label='Control')\n",
    "    ax[idx].axvline(threshold_0, color='black', alpha=1., ls='--', dashes=(1.5, 1.5), lw=1.2)\n",
    "    ax[idx].axvline(0, color='black', alpha=1., ls='-', lw=1.)\n",
    "    ax[idx].set_xlim(x_1_min, x_1_max)\n",
    "    ax[idx].set_xticks(())\n",
    "    ax[idx].set_xlabel('x: LR score')\n",
    "    #ax[idx].yaxis.set_visible(False)\n",
    "    #ax[idx].spines['left'].set_visible(False)\n",
    "    #ax[idx].legend(loc='lower center', bbox_to_anchor=(0.5, 0.95), frameon=False) #legend(loc=0, frameon=False)\n",
    "    ax[idx].title.set_text('outpatients')\n",
    "    ax[idx].grid(False)\n",
    "    \n",
    "\n",
    "plt.ylim(x_2_min, x_2_max)\n",
    "plt.yticks(())\n",
    "\n",
    "filename = 'ScatterplotLR#%s' % (model_name)\n",
    "filename = filename + exp_description + exp_multiv_description + '.pdf'\n",
    "saving_str = path_figures + filename\n",
    "plt.savefig(saving_str, bbox_inches='tight') #dpi=1000,\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnification = 0.85\n",
    "ratio = 2.4/3.7\n",
    "CF.SetPlotParams(magnification=magnification, ratio=ratio, \n",
    "                        fontsize=fontsize, ylabelsize=ylabelsize, xlabelsize=xlabelsize, \n",
    "                        tex=tex, axes_lines_w=axes_lines_w, lines_w=lines_w)\n",
    "mpl.rc('text', usetex = True)\n",
    "mpl.rc('text.latex', preamble=r'\\usepackage{sfmath}')\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.left'] = True\n",
    "mpl.rcParams['axes.spines.bottom'] = True\n",
    "mec = 'white'\n",
    "ms = 9\n",
    "mew = 0.15\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "value_pred = X2D_train[:, 0]\n",
    "y_pred = np.zeros_like(y_train)\n",
    "y_pred[value_pred>=threshold_0] = 1\n",
    "y_pred_1 = 1 - y_pred\n",
    "y_train_1 = 1 - y_train\n",
    "npv_model = precision_score(y_train_1, y_pred_1)\n",
    "spe_model = recall_score(y_train_1, y_pred_1)\n",
    "print('NPV: %.2f'%npv_model, '\\nSpec.: %.2f'%spe_model)\n",
    "    \n",
    "mask_class0 = y_train == 0\n",
    "mask_class1 = y_train == 1\n",
    "std = StandardScaler()\n",
    "x = X2D_train[:,0]\n",
    "x = std.fit_transform(x.reshape(-1, 1)).reshape(-1,)\n",
    "x0 = x[mask_class0]\n",
    "x1 = x[mask_class1]\n",
    "xcontrol = X2D_test[:, 0]\n",
    "xcontrol = std.transform(xcontrol.reshape(-1, 1)).reshape(-1,)\n",
    "x_min = min([np.nanmin(x0), np.nanmin(x1), np.nanmin(xcontrol)])\n",
    "x_max = max([np.nanmax(x0), np.nanmax(x1), np.nanmax(xcontrol)])\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 14)\n",
    "print('control: %d' % (round(100*np.sum(xcontrol<threshold_0)/len(xcontrol))))\n",
    "\n",
    "color = color_0\n",
    "density = False\n",
    "ax.hist(x0, bins,\n",
    "        alpha=0.15,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density)\n",
    "ax.hist(x0, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None', \n",
    "        label='no event')\n",
    "\n",
    "bins = np.linspace(x_min-0.2, x_max+0.2, 13)\n",
    "\n",
    "if 'FC' in model_name:\n",
    "        color = color_control\n",
    "        #ax.hist(x1, bins, alpha=.1,  color=color, density=density, edgecolor=color, label='event')\n",
    "        #ax.hist(x1, bins, alpha=1., edgecolor=color, density=density, facecolor='None')\n",
    "        ax.hist(xcontrol, bins,\n",
    "                alpha=0.2,\n",
    "                histtype='stepfilled',\n",
    "                color=color,\n",
    "                density=density,\n",
    "                label='control')\n",
    "        ax.hist(xcontrol, bins,\n",
    "                alpha=1.,\n",
    "                histtype='step', \n",
    "                edgecolor=color,\n",
    "                density=density,\n",
    "                facecolor='None')\n",
    "\n",
    "\n",
    "color = color_1\n",
    "#ax.hist(x1, bins, alpha=.1,  color=color, density=density, edgecolor=color, label='event')\n",
    "#ax.hist(x1, bins, alpha=1., edgecolor=color, density=density, facecolor='None')\n",
    "ax.hist(x1, bins,\n",
    "        alpha=0.2,\n",
    "        histtype='stepfilled',\n",
    "        color=color,\n",
    "        density=density,\n",
    "        label='event')\n",
    "ax.hist(x1, bins,\n",
    "        alpha=1., \n",
    "        histtype='step',\n",
    "        edgecolor=color,\n",
    "        density=density,\n",
    "        facecolor='None')\n",
    "\n",
    "color_boundary = CF.lighten_color(grey_1, 0.6)\n",
    "#ax.axvline(threshold_0, ls='-', lw=0.9, color=electricblue)\n",
    "ax.axvline(threshold_0, ls='--', lw=0.7, color='black') \n",
    "\n",
    "ax.title.set_text('npv=%.2f $\\,\\,\\,$ spe.=%.2f'%(npv_model, spe_model))\n",
    "ax.set_xlabel('%s score' % model_name)\n",
    "\n",
    "ax.set_ylabel('n. patients')\n",
    "if density:\n",
    "    ax.set_ylabel('patients (density)')\n",
    "\n",
    "if model_name=='FC+Dem':\n",
    "        ax.set_yticks([0., 50., 100., 150])\n",
    "        ax.set_ylim([0, 150])\n",
    "if model_name=='BM+Dem':\n",
    "        ax.set_ylim([0, 100])\n",
    "if model_name=='CK+Dem':\n",
    "       ax.set_ylim([0, 60])\n",
    "\n",
    "leg = ax.legend(loc='center left', bbox_to_anchor=(0.98, 0.5), frameon=False) #legend(loc=0, frameon=False)\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    lh.set_edgecolor('white')\n",
    "\n",
    "#fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axis\n",
    "#plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "#plt.xlabel('pc%d' % (comp1+1))\n",
    "#plt.ylabel('pc%d' % (comp2+1))\n",
    "\n",
    "filename = 'HistogramLR#%s' % (model_name)\n",
    "filename = filename + exp_description + exp_multiv_description + '.pdf'\n",
    "saving_str = path_figures + filename\n",
    "plt.savefig(saving_str, bbox_inches='tight') #dpi=1000,\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_6_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
